# 2020.01.27-2020.02.02

## Algorithm
### 1. 题目
```
二维数组中搜索（002）
```
### 2. 题目描述
```
二维数组中搜索

二维数组示例

[
  [1,   4,  7, 11, 15],
  [2,   5,  8, 12, 19],
  [3,   6,  9, 16, 22],
  [10, 13, 14, 17, 24],
  [18, 21, 23, 26, 30]
]


target = 5 , return true
target = 20 , return false

```

### 3. 解答：
```golang
func searchMatrix(matrix [][]int, target int) bool {
	if matrix == nil || len(matrix) == 0 || len(matrix[0]) == 0 {
		return false
	}
	row := 0
	col := len(matrix[0]) - 1
	for col >= 0 && row <= len(matrix)-1 {
		if matrix[row][col] == target {
			return true
		} else if matrix[row][col] > target {
			col--
		} else if matrix[row][col] < target {
			row++
		}
	}
	return false
}
```
### 4. 说明
略

## Review
### 1. 原文链接
[Partition Management in Hadoop](https://blog.cloudera.com/partition-management-in-hadoop/)

### 2. 翻译

Partition Management

分区管理

Well then, what exactly this storage management layer should do — is up to your specific problems. For instance, in our case there are 3 goals:

那么，实际上这个存储管理层应该做什么——取决于你具体的问题。例如，在我们的案例中有3个目标：

1. Merge partitions on selected tables

1. 在选中的表上合并分区

I want the “Partition Manager” to merge hourly partitions to monthly ones on a regular basis. 
The reason I’ve chosen monthly resolution as a merging standard is because it generates optimal-sized partitions (100mb-1gb). 
I don’t want one table to be partitioned monthly and the other yearly, for example, because I want to make it simple for our users (both analysts and data developers). 
The merging process will be described in detail later.

我希望“分区管理器”定期将每小时分区合并为每月分区。
我选择每月作为一个合并的标准的理由是因为这种方式生成最佳尺寸的分区（100mb-1gb）。
我不希望一个表按月度和年度进行分区，例如，因为我希望对我们的用户来说使他变得简单（分析师与数据开发者）。
这个合并过程稍后将详细描述。

2. Archiving cold data

2. 归档冷数据

Sometimes, we want to keep certain tables’ data for years, even though old data will probably be much less used. 
Therefore, I want my storage management layer to “archive” partitions that are older that 2 or 3 years (depends on your use-case of course). 
That is done by moving the data to another version of the table with a more aggressive compression algorithm like GZIP (compared to SNAPPY in the “hot” tables).

有时候，我们希望保存一些表的数据数年，尽管老数据会很少使用。
因此，我希望我们的存储管理层“归档”那些2年或3年的老数据。
这项工作是通过将数据移动到另一个版本的表，这个版本拥有更高效的压缩算法例如GZIP(相对于在“热”表中的SNAPPY)。

3. Delete partitions

3. 删除分区

And of course, we might want to choose a certain threshold (most probably a time threshold) for tables that we want to delete their old data from the HDFS. 
That is a really basic (but necessary) feature we would want from our storage management layer.

当然，我们可能希望为表选择一个确定的界限（极可能是一个时间界限）从HDFS中删除他们的老数据。
这是我们将希望来从存储管理层获得的一个最基本（但也是必需的）特性。

All of those 3 features are important, but I think the first one is the trickiest, and the actual reason I started writing this post, is to explain how I think it should be done.

这3个特性都很重要，但是我认为第一个是最棘手的，也是我开始写这篇文章的真正原因，就是解释我认为应该怎么做。

Partition Merging

分区合并

Well first of all, I’ll have to say that the complexity of this task really depends on your situation. 
In our situation, the task of merging partitions on a regular basis was not simple because of the following requirements:

首先，我不得不说这个任务的复杂性确实依赖于你的场景。
在我们得场景中，任务定期合并分区并简单，因为有以下要求：

Production tables with zero tolerance to downtime. Especially not if it’s more than a few seconds.

生产环境的表对停机是零容忍的。尤其是超过几秒钟。

Not losing time resolution — we found out some tables are partitioned by DT but there is no other matching time column.

不丢失时间精度——我们发现有些表被DT分区，但没有其他匹配的时间序列。

It means that if we are going to merge “2018041312” to “201804”, the users lose the daily and hourly time resolution on those partitions.

这意味着如果我们将要合并“2018041312”到“201804”，在那些分区中用户丢失了每日和每小时的时间精度。

As seamless as possible — the goal was to make the partition merging seamless to the users. 
We found out that in some cases it’s simply impossible with the current partitioning method (flat string DT), but in a different partitioning method, it’s quite possible. More on that later.

尽可能无缝的 - 目标是使分区合并与用户无缝。
我们发现，在某些情况下，使用当前的分区方法（扁平字符串DT）是不可能的，但是在不同的分区方法中，这是完全可能的。稍后再谈。



- 核心词汇
`specific` 具体的
`basis` 原因、基础
`on a regular basis` 定期地
`certain` 确定、某些
`optimal-sized` 最佳尺寸
`threshold` 门槛


### 3. 点评
这是一篇关于Hadoop里的分区管理的文章，这篇只翻译了第二部分，主要说明了分区管理的一些方法， `resolution`这个单词没有找到很好地翻译，百度翻译成分辨率，但感觉不符合语境。

## Tip
### 分片集群机制及原理

1. MongoDB常见的部署架构
    * 单机版
    * 复制集
    * 分片集群
    
2. 为什么要使用分片集群
    * 数据容量日益增大，访问性能日渐下降，怎么破？
    * 新品上线异常火爆，如何支撑更多用户并发？
    * 单库已有10TB数据，恢复需要1-2天，如何加速？
    * 地理分布数据

3. 分片如何解决问题？
    银行交易表单内10亿笔资料超负荷运转，交易号 0-1,000,000,000
    
    把数据分成两两半，放到两个库里。
    
    交易号：0-500,000,000
    
    交易号：500,000,001-1,000,000,000
    
    mongodb 最多可以分成1024片
    
4. 完整的分片集群
    [!qr](./images/0127_t_1.png)
    
5. 分片集群解剖：路由节点mongos
    * 提供集群单一入口
    * 转发应用端请求
    * 选择合适数据节点进行读写
    * 合并多个数据节点的返回
    * 无状态
    * 建议至少两个
6. 分片集群解剖：配置节点mongod
    * 提供集群元数据存储
    * 分片数据分布的映射
    
    普通复制集架构
    | Lower | Upper | Shard |
    |-------|-------|-------|
    | 0     | 1000  | Shard0 |
    | 1001  | 2000  | Shard1 |
    
7. 分片集群解剖：数据节点mongod
    * 以复制集为单位
    * 横向扩展
    * 最大1024分片
    * 分片之间数据不重复
    * 所有分片在一起才可以完整工作
8. MongoDB分片集群特点
    * 应用全透明，无特殊处理
    * 数据自动均衡
    * 动态扩容，无须下线
    * 提供三种分片方式
9. 分片集群数据分布方式
    * 基于范围
    [!qr](./images/0127_t_2.png)
    * 基于Hash
    [!qr](./images/0127_t_3.png)
    * 基于zone / tag
    [!qr](./images/0127_t_4.png)
10. 小结
    * 分盘集群可以有效解决性能瓶颈及系统扩容问题
    * 分片额外消耗较多，管理复杂，尽量不要分片


## Share
### UNIX 50 年：KEN THOMPSON 的密码
[UNIX 50 年：KEN THOMPSON 的密码](https://coolshell.cn/articles/19996.html)
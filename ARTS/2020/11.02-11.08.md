# 2020.11.02-2020.11.08 

## Algorithm
### 1. 题目
```
912. 排序数组(采用堆排序)
```
### 2. 题目描述
```
给你一个整数数组 nums，请你将该数组升序排列。

 

示例 1：

输入：nums = [5,2,3,1]
输出：[1,2,3,5]
示例 2：

输入：nums = [5,1,1,2,0,0]
输出：[0,0,1,1,2,5]
 

提示：

1 <= nums.length <= 50000
-50000 <= nums[i] <= 50000
```

### 3. 解答：
```golang
// 堆排序
func heapSort(arr []int) []int {
	l := len(arr) - 1
	buildHeap(arr, l)
	for l >= 1 {
		swap(arr, 0, l)
		l = l - 1
		heapify(arr, l, 0)
	}
	return arr
}

// 建堆
func buildHeap(arr []int, n int) {
	for i := n / 2; i >= 0; i-- {
		heapify(arr, n, i)
	}
}

// 交换
func swap(arr []int, x, y int) {
	arr[x], arr[y] = arr[y], arr[x]
}

// 堆化
func heapify(a []int, n, i int) {
	for {
		left := i*2 + 1
		right := i*2 + 2
		var maxPos = i
		if left <= n && a[i] < a[left] {
			maxPos = left
		}
		if right <= n && a[maxPos] < a[right] {
			maxPos = right
		}
		if maxPos == i {
			break
		}
		swap(a, i, maxPos)
		i = maxPos
	}
}
```
### 4. 说明
1. 建堆
2. 将尾部节点与顶部节点交换并再次堆化

## Review
### 1. 原文链接
[An Introduction to Kafka](https://dzone.com/articles/a-begineers-approach-to-quotkafkaquot)

### 2. 翻译
An Introduction to Kafka

kafka 简介

What Is Kafka?

什么是kafka

In simple terms, Kafka is a messaging system that is designed to be fast, scalable, and durable. 
It is an open-source stream processing platform. 
Apache Kafka originated at LinkedIn and later became an open-source Apache project in 2011, then a first-class Apache project in 2012. 
Kafka is written in Scala and Java. 
It aims at providing a high-throughput, low-latency platform for handling real-time data feeds.

简而言之，kafka是一个被设计为快速、可扩展以及可持久化的消息系统。
它是一个开源的流处理平台。
Apache kafka起源于LinkedIn然后在2011年成为了一个开源的Apache项目，在2012年成为了顶级的Apache项目。
Kafka采用Scala与Java编写。
它的目标是提供一个高吞吐量、低延迟的平台来处理实时数据传输。

Getting Familiar With Kafka

熟悉 kafka

Apache describes Kafka as a distributed streaming platform that lets us:

Apache描述kafka是一个分布式流平台，它让我们：

Publish and subscribe to streams of records.
Store streams of records in a fault-tolerant way.
Process streams of records as they occur.

发布和订阅记录流。
用一种容错的方式存储记录流。
记录流出现时处理它们。

Why Kafka?

为什么要使用kafka?

In Big Data, an enormous volume of data is used. But how are we going to collect this large volume of data and analyze that data? 
To overcome this, we need a messaging system. That is why we need Kafka. 

在大数据中，我们已经使用了海量的数据。但是我们如何去收集并分析这些数据呢？

The functionalities that it provides are well-suited for our requirements, and thus we use Kafka for:

它提供的功能非常适合我们的需求，因此我们使用Kafka:

Building real-time streaming data pipelines that can get data between systems and applications.

构建能够在系统和应用程序之间获取数据的实时流数据管道。

Building real-time streaming applications to react to the stream of data.

构建试试流应用来对流数据做出反应。

What Is a Messaging System?

什么是消息系统？

A messaging system is a system that is used for transferring data from one application to another so that the applications can focus on data and not on how to share it. 
Kafka is a distributed publish-subscribe messaging system. 
In a publish-subscribe system, messages are persisted in a topic. 
Message producers are called publishers and message consumers are called subscribers. 
Consumers can subscribe to one or more topic and consume all the messages in that topic (we will discuss these terminologies later in the post).

消息系统是指用于将数据从一个应用传递到另一个的系统，以便应用能够关注数据而非如何共享数据。
kafka是一个分布式发布-订阅消息系统。
在一个发布-订阅系统中，消息被持久化在topic中。
消息生产这被发布者调用，消息消费者被关注着调用。
消费者可以关注一个或多个topic并且消费在该topic（我们稍后将在本文中讨论这些术语）的所有消息。

Benefits of Kafka

kafka的优点

Four main benefits of Kafka are:

kafka四大优点是：

Reliability. Kafka is distributed, partitioned, replicated, and fault tolerant. Kafka replicates data and is able to support multiple subscribers. 
Additionally, it automatically balances consumers in the event of failure.

可靠性。kafka是分布式的、分区的、重复的以及容错的。kafka复制数据并能够支持多个订阅者。
此外，在发送故障时它会自动平衡消费者。

Scalability. Kafka is a distributed system that scales quickly and easily without incurring any downtime.
可扩展性。kafka是一个快速而轻松地扩展，不会导致任何停机的分布式系统。

Durability. Kafka uses a distributed commit log, which means messages persists on disk as fast as possible providing intra-cluster replication, hence it is durable.
持久性。kafka使用分布式提交日志，这意味着通过提供集群内复制，消息尽可能快地保存在磁盘上，因此它是持久的。

Performance. Kafka has high throughput for both publishing and subscribing messages. It maintains stable performance even when dealing with many terabytes of stored messages.
高性能。kafka对于发布和订阅消息都具有高吞吐量。即使在处理存储的许多tb的消息时，它也能保持稳定的性能。

Now, we can move on to our next step: Kafka basics.

现在我们可以进入下个步奏：kafka基础知识。

Basics of Kafka

kafka的基础知识

Apache.org states that:

Apache.org描述如下：

Kafka runs as a cluster on one or more servers.
The Kafka cluster stores a stream of records in categories called topics.
Each record consists of a key, a value, and a timestamp.

kafka在一台服务器或多台呋尔奇运行一个集群。
kafka集群存储记录流在叫作topic的分类中。
每条记录由一个key一个value和一个timestamp组成。

Topics and Logs

主题和日志

A topic is a feed name or category to which records are published. 

主题是将记录发布到的提要名称或类别。

Topics in Kafka are always multi-subscriber — that is, a topic can have zero, one, or many consumers that subscribe to the data written to it. 

Kafka中的主题总是多订阅者——也就是说，一个主题可以有零个、一个或多个订阅写入到它的数据的消费者。

For each topic, the Kafka cluster maintains a partition log that looks like this:

对于每个主题，Kafka集群维护一个分区日志，如下所示:

[!qr](./images/1102_r_1.png)

Partitions

分区

A topic may have many partitions so that it can handle an arbitrary amount of data. 
In the above diagram, the topic is configured into three partitions (partition{0,1,2}). 
Partition0 has 13 offsets, Partition1 has 10 offsets, and Partition2 has 13 offsets.

一个主题可能拥有许多分区因此它可以处理任意数量的数据。
在上图中，主题被配置为三个分区（分区｛0,1,2｝）。
分区0有13个偏移量，分区1有10个偏移量，分区2有13个偏移量。

Partition Offset

分区偏移量

Each partitioned message has a unique sequence ID called an offset. For example, in Partition1, the offset is marked from 0 to 9.

每个分区消息拥有一个唯一的号ID被称为偏移量。例如，在分区1中，偏移量被标记为0到9。

Replicas

复制集

Replicas are nothing but backups of a partition. 
If the replication factor of the above topic is set to 4, then Kafka will create four identical replicas of each partition and place them in the cluster to make them available for all its operations. 
Replicas are never used to read or write data. They are used to prevent data loss.

复制集是分区的备份。
如果上述主题的复制因子设置为4，那么Kafka将为每个分区创建4个相同的副本，并将它们放在集群中，以使它们可用于集群的所有操作。
复制集永远不会用于读或写数据。它们用于防止数据丢失。

Brokers
Brokers are simple systems responsible for maintaining published data. 
Kafka brokers are stateless, so they use ZooKeeper for maintaining their cluster state. Each broker may have zero or more partitions per topic. 
For example, if there are 10 partitions on a topic and 10 brokers, then each broker will have one partition. But if there are 10 partitions and 15 brokers, 
then the starting 10 brokers will have one partition each and the remaining five won’t have any partition for that particular topic. 
However, if partitions are 15 but brokers are 10, then brokers would be sharing one or more partitions among them, leading to unequal load distribution among the brokers. 
Try to avoid this scenario.

代理

代理是负责维护已发布的数据的简单系统。
kafka 代理是无状态的，因此他们使用ZooKeeper维护他们的集群状态。每个代理每个主题中可能拥有0个或多个分区。
例如，如果一个主题上有10个分区和10个代理，那么每个代理将有一个分区。但是，如果有10个分区和15个代理，那么开始运行的10个代理将各有一个分区，其余5个代理将没有针对该特定主题的分区。
但是，如果分区是15个，而代理是10个，那么代理将在它们之间共享一个或多个分区，从而导致代理之间的负载分配不均等。
尽量避免这种情况。

Zookeeper

Zookeeper

ZooKeeper is used for managing and coordinating Kafka brokers. 
ZooKeeper is mainly used to notify producers and consumers about the presence of any new broker in the Kafka system or about the failure of any broker in the Kafka system. 
ZooKeeper notifies the producer and consumer about the presence or failure of a broker based on which producer and consumer makes a decision and starts coordinating their tasks with some other broker.

ZooKeeper 用于管理和协调Kafka代理的。
ZooKeeper 主要用于通知生产者和消费者关于卡夫卡系统中出现的任何新经纪人或卡夫卡系统中任何经纪人的故障。
ZooKeeper 通知生产者和消费者代理的存在或失败，生产者和消费者将根据该代理做出决策，并开始与其他一些代理协调其任务。


Cluster

集群

When Kafka has more than one broker, it is called a Kafka cluster. A Kafka cluster can be expanded without downtime. 
These clusters are used to manage the persistence and replication of message data.

当Kafka有多个代理时，它被称为Kafka集群。可以在不停机的情况下扩展Kafka集群。
这些集群用于管理消息数据的持久性和复制。

Kafka has four core APIs:

Kafka拥有四大核心API:

The Producer API allows an application to publish a stream of records to one or more Kafka topics.

生产者API允许应用发布记录流到一个或多个Kafka主题。

The Consumer API allows an application to subscribe to one or more topics and process the stream of records produced to them.

消费者API允许应用关注一个或多个主题并处理为它们生产的记录流。

The Streams API allows an application to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.

流API允许应用程序充当流处理器，使用来自一个或多个主题的输入流并生成一个或多个输出主题的输出流，有效地将输入流转换为输出流。

The Connector API allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table.

连接器API允许构建和运行可重用的生产者或消费者，将Kafka主题连接到现有的应用程序或数据系统。例如，连接到关系数据库的连接器可能捕获对表的每个更改。

-核心词汇
    * occur 出现
    * basics 基础知识
    


### 3. 点评
这是一篇关于kafka的简要介绍，作为消息平台的一哥，kafka的架构与设计相当优秀。

## Tip
### golang select
1. select语句中除default外，每个case操作一个channel，要么读要么写
2. select语句中除default外，各case执行顺序是随机的
3. select语句中如果没有default语句，则会阻塞等待任一case
4. select语句中读写操作要判断是否成功读取，关闭的channel也可以读取

## Share
###  gRPC长连接在微服务业务系统中的实践
[gRPC长连接在微服务业务系统中的实践](https://www.infoq.cn/article/cPXR35BwJttgNCLtYEKZ)